{
    "learning_rate": 1e-3,
    "batch_size": 5,
    "num_epochs": 50,
    "dropout": 0.3,
    "bidirectional": 1,
    "n_layers":2,
    "lstm_hidden_dim": 40,
    "embedding_dim": 20,
    "save_summary_steps": 100,
    "attention": 1,
    "attention_type": "regular",
    "attention_size": 512,
    "attn" : 1
}
